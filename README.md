# Final Project

In this project, I run a baseline from NailP based on their 2022 results in CLEF eRisk: `sentence-transformers/all-MiniLM-L6-v2`.
Following up, I make some modifications to the baseline by running a finetuned distilbert model from a huggingface contributor: 
`bhadresh-savani/distilbert-base-uncased-emotion`. 

The majority of the project code may be found in the jupyter notebooks in the root directory. 
One limitation for users is that they are currently only configured to run with CUDA-enabled machines. 

The order to run the notebooks is 1. Baseline Run, 2. Post-Baseline Run, 3. Metrics and Comparisons.

## CSV Files

`tabulated_unfiltered_trec.csv`: This data is all user posts by id, supplied by eRisk organizers and organized into a csv file.

`tabulated_cleaned_unfiltered_trec.csv`: This data is user posts filtered so that empty posts and other noise are removed.

`tabulated_cleaned_emotionfiltered_trec.csv`: This data is user posts by id, filtered so that only self-referential posts with negative emotions are included (nltk was used to detect emotion polarity).

`ranking_augmented_data.csv`: This is the csv with all the ranking scores (cosine sim) from the baseline model run - all-Mini-LM.

`augmented_answer_sets.csv`: Answer sets generated by the NailP team and provided in their publication. The size of the sample answer sets from the original BDI is increased tenfold.

## TSV files

Files for the second run will be in TSV format - it is generally better practice to store files in TSV format for these tasks.

`minilm_results.tsv`: "all-Mini-LM-L6-v2" results (our base model) -> trec-formatted output.

`distilbert_mod_results.tsv`: "distilbert-base-uncased-emotion" results (non-base model) -> trec-formatted output.

## Notebooks

`Baseline_Run.ipynb`: The notebook for the baseline run of the model - using the all-MiniLM-L6-v2 sentence transformer.

`Post_Baseline_Run.ipynb`: Modified to produce results from distillbert-base-uncased-emotion.

`Metrics_and_Comparisons.ipynb`: Generate basic retrieval effectiveness metrics for the models 

## Python (.py) Files

`main.py`: Incomplete. Preprocessing steps for trec-formatted input data.

## Directories
`task1`: The folder provided by the team at CLEF eRisk containing training and evaluation data.
`output_plots`: Plots for comparison of outputs between the baseline and fine-tuned model, based on majority qrels.
