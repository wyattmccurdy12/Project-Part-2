{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for the purpose of running the `distillbert base uncased emotion` model hosted on huggingface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bhadresh-savani/distilbert-base-uncased-emotion. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertModel\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('bhadresh-savani/distilbert-base-uncased-emotion')\n",
    "model = SentenceTransformer('bhadresh-savani/distilbert-base-uncased-emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean emotion filtered data shape:  (27981, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s_1287_153_9</td>\n",
       "      <td>I mean what the hell bro.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s_1287_187_0</td>\n",
       "      <td>Yeah, crazy isn't it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s_1287_204_0</td>\n",
       "      <td>No :( sadly it doesn't have everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s_1287_222_4</td>\n",
       "      <td>I'm worried.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s_1287_240_1</td>\n",
       "      <td>Better weapons and going against a weaker team...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          docid                                               TEXT\n",
       "0  s_1287_153_9                          I mean what the hell bro.\n",
       "1  s_1287_187_0                              Yeah, crazy isn't it?\n",
       "2  s_1287_204_0           No :( sadly it doesn't have everything  \n",
       "3  s_1287_222_4                                       I'm worried.\n",
       "4  s_1287_240_1  Better weapons and going against a weaker team..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "clean_ef_data = pd.read_csv('tabulated_cleaned_emotionfiltered_trec.csv')\n",
    "clean_ef_data = clean_ef_data.drop(['polarity', 'self_ref', 'PRE', 'POST'], axis=1)\n",
    "print(\"clean emotion filtered data shape: \", clean_ef_data.shape)\n",
    "clean_ef_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aug answer set shape:  (928, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>I don’t sleep as well as I used to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>My appetite is significantly reduced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>I haven’t lost much weight (if any) lately</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>I haven’t seen much change in my weight lately</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Question  Severity                                             Text\n",
       "675        16         2               I don’t sleep as well as I used to\n",
       "780        18         3             My appetite is significantly reduced\n",
       "796        19         1       I haven’t lost much weight (if any) lately\n",
       "798        19         1   I haven’t seen much change in my weight lately"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load set of augmented answers\n",
    "aug_answer_set = pd.read_csv('augmented_answer_sets.csv')\n",
    "print(\"aug answer set shape: \", aug_answer_set.shape)\n",
    "aug_answer_set.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have answer sets and input text, let's create embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2950f0cc999a42d886859b76bce52cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create tokens and vector embeddings for user posts\n",
    "post_embeddings_df = clean_ef_data.copy()\n",
    "\n",
    "post_embeddings = post_embeddings_df['TEXT'].to_list()\n",
    "\n",
    "post_embeddings = model.encode(post_embeddings, device='cuda', show_progress_bar=True, \n",
    "                               output_value='sentence_embedding', convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 50.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to store the embeddings\n",
    "bdi_embeddings = {}\n",
    "\n",
    "# Loop over all 21 BDI questions\n",
    "for i in tqdm(range(1, 22), total=21):\n",
    "    # Filter the DataFrame for the current question and severity > 1\n",
    "    bdi_i_embedding_df = aug_answer_set[(aug_answer_set['Question'] == i) & (aug_answer_set['Severity'] > 1)]\n",
    "    \n",
    "    # Get embeddings for the filtered DataFrame\n",
    "    bdi_i_embeddings = model.encode(\n",
    "        bdi_i_embedding_df['Text'].to_list(), device='cuda', output_value='sentence_embedding', convert_to_tensor=True\n",
    "    )\n",
    "    \n",
    "    # Store the embeddings in the dictionary\n",
    "    bdi_embeddings[f\"q{i}\"] = bdi_i_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have embeddings for each post and the associated question, we will rank. \n",
    "\n",
    "The rankings will be computed for each question, and are based on the max-similarity \n",
    "\n",
    "between a post's embedding and the embedding array of the corresponding question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27981/27981 [00:03<00:00, 9119.83it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9517.19it/s]\n",
      "100%|██████████| 27981/27981 [00:03<00:00, 9319.21it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9428.92it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9448.21it/s]\n",
      "100%|██████████| 27981/27981 [00:03<00:00, 9119.29it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9499.36it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9516.21it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9530.54it/s]\n",
      "100%|██████████| 27981/27981 [00:03<00:00, 9118.23it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9506.37it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9507.12it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9510.92it/s]\n",
      "100%|██████████| 27981/27981 [00:03<00:00, 9122.39it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9495.33it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9500.83it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9515.45it/s]\n",
      "100%|██████████| 27981/27981 [00:03<00:00, 9135.37it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9519.91it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9526.55it/s]\n",
      "100%|██████████| 27981/27981 [00:02<00:00, 9534.14it/s]\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity_dict = {}\n",
    "\n",
    "for i in range(1, 22):\n",
    "\n",
    "    # Get the correct embeddings list by key\n",
    "    qa_embeddings = bdi_embeddings[f\"q{i}\"]\n",
    "\n",
    "    # Get the max cosine similarity between each post embedding and the answer set for the current BDI question\n",
    "    qi_cos_similarities = [\n",
    "        torch.max(cosine_similarity(post_embedding, qa_embeddings)).item()\n",
    "        for post_embedding in tqdm(post_embeddings, total=len(post_embeddings))\n",
    "    ]\n",
    "\n",
    "    # Assign these max cosine similarity rankings to the cosine similarity dictionary\n",
    "    cosine_similarity_dict[f'q{i}'] = qi_cos_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cosine similarities for each question, we need to assign them post ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 136.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to store the rankings for each question\n",
    "rankings_dict = {}\n",
    "\n",
    "# Loop over all 21 BDI questions\n",
    "for i in tqdm(range(1, 22), total=21):\n",
    "    # Make a copy of the clean_ef_data DataFrame\n",
    "    q_rankings = clean_ef_data.copy()\n",
    "    \n",
    "    # Add a 'score' column to the DataFrame, which contains the cosine similarity scores for the current question\n",
    "    q_rankings['score'] = cosine_similarity_dict[f'q{i}']\n",
    "    \n",
    "    # Sort the DataFrame by the 'score' column in descending order and reset the index\n",
    "    q_rankings = q_rankings.sort_values('score', ascending=False, ignore_index=True)\n",
    "    \n",
    "    # Keep only the top 1000 rows\n",
    "    q_rankings = q_rankings.head(1000)\n",
    "    \n",
    "    # Store the DataFrame in the rankings_dict dictionary with the key as the current question\n",
    "    rankings_dict[f'q{i}'] = q_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the rankings dict as trec-formatted content before saving to disk.\n",
    "for i, key in enumerate(rankings_dict.keys()):\n",
    "    rankings_dict[key]['query'] = f'{i+1}' # create query number\n",
    "    rankings_dict[key]['q_id'] = rankings_dict[key]['query']\n",
    "    rankings_dict[key]['doc_id'] = rankings_dict[key]['docid']\n",
    "    rankings_dict[key]['q0'] = '0'\n",
    "    rankings_dict[key]['rank'] = range(1, 1001)\n",
    "    rankings_dict[key]['rank'] = rankings_dict[key]['rank'].astype(str)\n",
    "    rankings_dict[key]['model'] = 'distilbert-base-uncased-emotion'\n",
    "    rankings_dict[key] = rankings_dict[key][[\"q_id\", \"q0\", \"doc_id\", \"score\", \"rank\", \"model\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, df in rankings_dict.items():\n",
    "#     df.to_csv(f'distilbert-base-uncased-ranking-outputs/{key}_rankings.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate all the DataFrames in the dictionary\n",
    "# all_rankings = pd.concat(rankings_dict.values(), ignore_index=True)\n",
    "\n",
    "# # Save the concatenated DataFrame as a TSV file\n",
    "# all_rankings.to_csv('dbue_results.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have completed the rankings portion. We need to compare rankings against our qrels to get performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import Run, Qrels, evaluate\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the tsv file\n",
    "# distilbert_rankings = pd.read_csv(\"dbue_results.tsv\", sep=\"\\t\")\n",
    "# distilbert_rankings['query'] = distilbert_rankings['query'].astype(str)\n",
    "# distilbert_rankings.columns = [\"q_id\", \"q0\", \"doc_id\", \"score\", \"rank\", \"model\"]\n",
    "# distilbert_rankings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>q0</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>s_405_1279_15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>s_2519_356_0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>s_2038_51_7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>s_975_61_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>s_577_923_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  q_id  q0         doc_id  score\n",
       "0    1   0  s_405_1279_15      1\n",
       "1    1   0   s_2519_356_0      0\n",
       "2    1   0    s_2038_51_7      1\n",
       "3    1   0     s_975_61_2      1\n",
       "4    1   0    s_577_923_1      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels_majority = pd.read_csv('task1/training/t1_training/TRAINING DATA (2023 COLLECTION)/g_qrels_majority_2.csv')\n",
    "qrels_majority['query'] = qrels_majority['query'].astype(str)\n",
    "qrels_majority.columns = [\"q_id\", \"q0\", \"doc_id\", \"score\"]\n",
    "qrels_majority.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the ground truth and predicted rankings\n",
    "\n",
    "# run = Run.from_df(distilbert_rankings)\n",
    "\n",
    "# # # Compute the evaluation metrics\n",
    "# results = evaluate(qrels, run, metrics=['r-precision', 'precision@10', 'map', 'ndcg@1000'])\n",
    "\n",
    "# # # Print the results\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull one ranking each from rankings_dict. Compare each against majority qrels to get metrics\n",
    "result_collection = []\n",
    "qrels = Qrels.from_df(qrels_majority)\n",
    "for i, q in enumerate(rankings_dict.keys()):\n",
    "    # Handle ranking scoring dataframes one at a time\n",
    "    q_rank_df = rankings_dict[q]\n",
    "\n",
    "    # Get the ranking against the qrels majority file\n",
    "    run = Run.from_df(q_rank_df)\n",
    "    \n",
    "    results = evaluate(qrels, run, metrics=['r-precision', 'precision@10', 'map', 'ndcg@1000'], make_comparable=True)\n",
    "    results['q_id'] = str(i+1) \n",
    "    result_collection.append(results)\n",
    "\n",
    "results_df = pd.DataFrame(result_collection)\n",
    "# Compute the mean of each column\n",
    "\n",
    "\n",
    "results_df.to_csv(\"finetuned_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
